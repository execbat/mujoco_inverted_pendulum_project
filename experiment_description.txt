Environment Documentation: MuJoCo Inverted Pendulum Target

Overview

	This project defines a custom OpenAI Gym-compatible environment built upon the MuJoCo physics simulator. The environment simulates an inverted pendulum mounted on a cart. The control task for the agent is to move the pole's tip to a dynamically spawned target point in space and maintain balance in its vicinity. The environment is designed for use with reinforcement learning algorithms, in particular Proximal Policy Optimization (PPO).
	

Core Objective

	The main learning objective is to control the cart such that:

	The pole tip reaches a target position (a randomly spawned point referred to as a "ball").

	The pole remains balanced in an upright position.

	This dual objective promotes both precision in reaching the spatial target and dynamic stabilization.
	

Environment Architecture

	The environment is implemented using gym.Env and is compatible with the Gymnasium API. It uses a MuJoCo-based physics simulation (via InvertedPendulumEnv) and integrates seamlessly with standard RL pipelines.
	

Action Space

	Type: Box(low=-3.0, high=3.0, shape=(1,), dtype=np.float32)

	Description: Represents the continuous horizontal force applied to the cart.
	

Observation Space

	Type: Box(shape=(14,))

	Contents:

	Cart position and velocity

	Pole angle and angular velocity

	Arc distance between the pole tip and the vertical point above the cart

	Euclidean distance between the pole tip and the target point

	Pole tip X, Z position in world coordinates

	Target X, Z position in world coordinates

	Pole mass (normalized)

	Gravity moment (torque due to gravity)

	Linear velocity magnitude of the pole tip

	Acceleration of the pole tip



Reward Design

	The reward function combines several shaped components:

	Positive Rewards

		Tip Speed Bonus: Encourages the pole tip velocity to approximate a predefined function based on the arc angle (imitation-based guidance).

		Tip Acceleration Bonus: Encourages acceleration to match the derivative of the velocity function.

		Upright Bonus: Multiplies rewards by (1 + cos(theta)) / 2, increasing the reward when the pole is near vertical.

		Gravity Moment Compensation Bonus: Rewards when the torque due to gravity is low near the vertical position.

		Distance-to-Target Bonus: Rewards proximity to the target (ball), scaled more heavily when near vertical.

	Penalties

		Cart Out of Bounds: Penalizes if the cart leaves screen bounds.

		Inactivity: Penalizes minimal state change between steps.

		(Optional) High-Speed Penalty: Can be activated to discourage excessive pole tip velocity.
		

Domain Randomization

	To enhance robustness, the pole mass is randomized within [10.0, 30.0] kg on each reset. The normalized mass is included in the observation to allow the policy to adapt dynamically.
	

Learning Strategy

	The environment is tailored for model-free reinforcement learning. PPO (Proximal Policy Optimization) is the main algorithm applied. The reward shaping acts as curriculum guidance:

	The agent learns to generate upward motion using velocity and acceleration matching.

	It stabilizes the pole vertically.

	It learns to navigate and hold near dynamically spawned target points.
	

Termination

	Episodes terminate upon cart boundary violations, or when predefined simulation constraints are breached.
	

Rendering

	MuJoCo visualizer can be enabled by setting render=True.
	

Usage

	Training: via train_cpu.py, train_gpu.py, or train_appo_gpu.py

	Evaluation: via enjoy.py

	Logging: TensorBoard logs are stored under runs/ppo_run

	Configuration: specified in config.yaml
	

Conclusion

	This environment provides a physically grounded, modular, and curriculum-driven framework for training reinforcement learning agents on precision balancing and control tasks. By combining domain randomization, imitation-like signal shaping, and well-structured rewards, it serves as a rich testbed for control algorithms in dynamic environments.


